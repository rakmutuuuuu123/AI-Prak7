# -*- coding: utf-8 -*-
"""JST_G.231.20.0175.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17RKhlHTdxODIiYKgqoteoPm-e3Kpv-wa
"""

import keras
import numpy as np
from sklearn.datasets import load_iris
dataset=load_iris()
##How data looks like?
print(dataset)

##Step : 1 : Know the data
print(type(dataset))
print(len(dataset))
#Dataset is in bunch format. Bunch is a dictionary like object.
print(dataset.keys())
#We try to find out what values are stored in these attributes
print(dataset['data'][0:5])
print(dataset['target'][0:5])
print(dataset['target_names'][0:5])
print(dataset['DESCR'])
print(dataset['feature_names'])
print(dataset['filename'])
#Next we define our x and y; y is the dependent on x
x=dataset['data']
print(len(x)) #To check length of dataset
y=dataset['target']
print(len(y)) #To check length of targets
print(x[0])
print(y[0])

# Step 2: Convert y into one hot encoded vector
# One hot encoding is used because Y has labels (0, 1, 2) - which is categorical data with no ordinal relationships
from keras.utils import to_categorical
Ny = len(np.unique(y))
print(Ny)
Y = to_categorical(y, num_classes=Ny)
print(Y[0:5])

##Step : 3 : Now we split the data into two parts - one for training another for testing
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,Y,test_size=0.10,shuffle=True)
print(x_train[0:5])
print(x_test[0:5])
print(y_train[0:5])
print(y_test[0:5])

##Step : 4 : Then we normalize the data
from sklearn.preprocessing import StandardScaler
##Normalization is done so that the difference between highest and lowest data point is not too large
import numpy as np
scaler=StandardScaler()
##To find mean and std dev
scaler.fit(x_train)

##Converting data into form where mean of data is 0 and std dev is 1
X_train=scaler.transform(x_train)
X_test=scaler.transform(x_test)
print(np.amax(X_train,axis=0))
print(np.amin(X_train,axis=0))

##Step : 5 : Now finally we build the model using keras
!pip install tensorflow
import keras
from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(20, input_dim=X_train.shape[1], activation='relu'))
##Dropout is used to avoid overfitting
keras.layers.Dropout(0.2)
model.add(Dense(20, activation='relu'))
keras.layers.Dropout(0.2)
model.add(Dense(20, activation='relu'))
keras.layers.Dropout(0.2)
##For classification problems, we usually use softmax as activation function in final layer
model.add(Dense(3, activation='softmax'))
##Compiling the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

##Step : 6 : Know the model
model.summary()

##Step : 7 : Train the model
##Batch size, epoch and validation split are all hyper parameters
model.fit(X_train, y_train, epochs=195, batch_size=80, validation_split=0.1)

##Step : 8 : Get the accuracy of model on testing data
testing=model.evaluate(X_test, y_test)
print("\n%s: %.2f%%" % (model.metrics_names[1]+'uracy of Model on testing data', testing[1]*100))

##Step : 9 : Evaluate our model
from sklearn.metrics import classification_report, confusion_matrix
predictions = np.argmax(model.predict(X_test), axis=1)
Y_test = np.argmax(y_test,axis=1)
#To get confusion matrix
print(confusion_matrix(Y_test,predictions))
#To get values of all evaluation metrics
print(classification_report(Y_test,predictions))